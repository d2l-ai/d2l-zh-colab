{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6fca4b",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74798f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U mxnet-cu101==1.7.0\n",
    "!pip install git+https://github.com/d2l-ai/d2l-zh@release  # installing d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2ad3a",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 自动并行\n",
    ":label:`sec_auto_para`\n",
    "\n",
    "深度学习框架（例如，MxNet、飞桨和PyTorch）会在后端自动构建计算图。利用计算图，系统可以了解所有依赖关系，并且可以选择性地并行执行多个不相互依赖的任务以提高速度。例如， :numref:`sec_async`中的 :numref:`fig_asyncgraph`独立初始化两个变量。因此，系统可以选择并行执行它们。\n",
    "\n",
    "通常情况下单个操作符将使用所有CPU或单个GPU上的所有计算资源。例如，即使在一台机器上有多个CPU处理器，`dot`操作符也将使用所有CPU上的所有核心（和线程）。这样的行为同样适用于单个GPU。因此，并行化对单设备计算机来说并不是很有用，而并行化对于多个设备就很重要了。虽然并行化通常应用在多个GPU之间，但增加本地CPU以后还将提高少许性能。例如， :cite:`Hadjis.Zhang.Mitliagkas.ea.2016`则把结合GPU和CPU的训练应用到计算机视觉模型中。借助自动并行化框架的便利性，我们可以依靠几行Python代码实现相同的目标。对自动并行计算的讨论主要集中在使用CPU和GPU的并行计算上，以及计算和通信的并行化内容。\n",
    "\n",
    "请注意，本节中的实验至少需要两个GPU来运行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f48157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:58:01.672454Z",
     "iopub.status.busy": "2022-12-07T14:58:01.672117Z",
     "iopub.status.idle": "2022-12-07T14:58:05.164662Z",
     "shell.execute_reply": "2022-12-07T14:58:05.163831Z"
    },
    "origin_pos": 1,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "from d2l import mxnet as d2l\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6f969",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## 基于GPU的并行计算\n",
    "\n",
    "从定义一个具有参考性的用于测试的工作负载开始：下面的`run`函数将执行$10$次*矩阵－矩阵*乘法时需要使用的数据分配到两个变量（`x_gpu1`和`x_gpu2`）中，这两个变量分别位于选择的不同设备上。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169ecfbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:58:05.168651Z",
     "iopub.status.busy": "2022-12-07T14:58:05.168280Z",
     "iopub.status.idle": "2022-12-07T14:58:13.177299Z",
     "shell.execute_reply": "2022-12-07T14:58:13.176155Z"
    },
    "origin_pos": 5,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [],
   "source": [
    "devices = d2l.try_all_gpus()\n",
    "def run(x):\n",
    "    return [x.dot(x) for _ in range(50)]\n",
    "\n",
    "x_gpu1 = np.random.uniform(size=(4000, 4000), ctx=devices[0])\n",
    "x_gpu2 = np.random.uniform(size=(4000, 4000), ctx=devices[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f50d77",
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "现在使用函数来处理数据。通过在测量之前需要预热设备（对设备执行一次传递）来确保缓存的作用不影响最终的结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0999c86e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:58:13.181324Z",
     "iopub.status.busy": "2022-12-07T14:58:13.181043Z",
     "iopub.status.idle": "2022-12-07T14:58:14.724891Z",
     "shell.execute_reply": "2022-12-07T14:58:14.724068Z"
    },
    "origin_pos": 11,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU1 时间: 0.5008 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU2 时间: 0.5144 sec\n"
     ]
    }
   ],
   "source": [
    "run(x_gpu1)  # 预热设备\n",
    "run(x_gpu2)\n",
    "npx.waitall()\n",
    "\n",
    "with d2l.Benchmark('GPU1 时间'):\n",
    "    run(x_gpu1)\n",
    "    npx.waitall()\n",
    "\n",
    "with d2l.Benchmark('GPU2 时间'):\n",
    "    run(x_gpu2)\n",
    "    npx.waitall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c525e8",
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "如果删除两个任务之间的`waitall`语句，系统就可以在两个设备上自动实现并行计算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f817bc78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:58:14.729379Z",
     "iopub.status.busy": "2022-12-07T14:58:14.729105Z",
     "iopub.status.idle": "2022-12-07T14:58:15.255225Z",
     "shell.execute_reply": "2022-12-07T14:58:15.254360Z"
    },
    "origin_pos": 17,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU1 & GPU2: 0.5218 sec\n"
     ]
    }
   ],
   "source": [
    "with d2l.Benchmark('GPU1 & GPU2'):\n",
    "    run(x_gpu1)\n",
    "    run(x_gpu2)\n",
    "    npx.waitall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffef796",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "在上述情况下，总执行时间小于两个部分执行时间的总和，因为深度学习框架自动调度两个GPU设备上的计算，而不需要用户编写复杂的代码。\n",
    "\n",
    "## 并行计算与通信\n",
    "\n",
    "在许多情况下，我们需要在不同的设备之间移动数据，比如在CPU和GPU之间，或者在不同的GPU之间。例如，当执行分布式优化时，就需要移动数据来聚合多个加速卡上的梯度。让我们通过在GPU上计算，然后将结果复制回CPU来模拟这个过程。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c654128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:58:15.258796Z",
     "iopub.status.busy": "2022-12-07T14:58:15.258212Z",
     "iopub.status.idle": "2022-12-07T14:58:18.406478Z",
     "shell.execute_reply": "2022-12-07T14:58:18.405370Z"
    },
    "origin_pos": 21,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在GPU1上运行: 0.7555 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "复制到CPU: 2.3861 sec\n"
     ]
    }
   ],
   "source": [
    "def copy_to_cpu(x):\n",
    "    return [y.copyto(npx.cpu()) for y in x]\n",
    "\n",
    "with d2l.Benchmark('在GPU1上运行'):\n",
    "    y = run(x_gpu1)\n",
    "    npx.waitall()\n",
    "\n",
    "with d2l.Benchmark('复制到CPU'):\n",
    "    y_cpu = copy_to_cpu(y)\n",
    "    npx.waitall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759090a5",
   "metadata": {
    "origin_pos": 24,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "这种方式效率不高。注意到当列表中的其余部分还在计算时，我们可能就已经开始将`y`的部分复制到CPU了。例如，当计算一个小批量的梯度时，某些参数的梯度将比其他参数的梯度更早可用。因此，在GPU仍在运行时就开始使用PCI-Express总线带宽来移动数据是有利的。删除这两个部分之间的`waitall`以模拟这个场景。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1c5054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:58:18.410563Z",
     "iopub.status.busy": "2022-12-07T14:58:18.409969Z",
     "iopub.status.idle": "2022-12-07T14:58:20.871472Z",
     "shell.execute_reply": "2022-12-07T14:58:20.870273Z"
    },
    "origin_pos": 27,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在GPU1上运行并复制到CPU: 2.4557 sec\n"
     ]
    }
   ],
   "source": [
    "with d2l.Benchmark('在GPU1上运行并复制到CPU'):\n",
    "    y = run(x_gpu1)\n",
    "    y_cpu = copy_to_cpu(y)\n",
    "    npx.waitall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d3457",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "两个操作所需的总时间少于它们各部分操作所需时间的总和。请注意，与并行计算的区别是通信操作使用的资源：CPU和GPU之间的总线。事实上，我们可以在两个设备上同时进行计算和通信。如上所述，计算和通信之间存在的依赖关系是必须先计算`y[i]`，然后才能将其复制到CPU。幸运的是，系统可以在计算`y[i]`的同时复制`y[i-1]`，以减少总的运行时间。\n",
    "\n",
    "最后，本节给出了一个简单的两层多层感知机在CPU和两个GPU上训练时的计算图及其依赖关系的例子，如 :numref:`fig_twogpu`所示。手动调度由此产生的并行程序将是相当痛苦的。这就是基于图的计算后端进行优化的优势所在。\n",
    "\n",
    "![在一个CPU和两个GPU上的两层的多层感知机的计算图及其依赖关系](http://d2l.ai/_images/twogpu.svg)\n",
    ":label:`fig_twogpu`\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 现代系统拥有多种设备，如多个GPU和多个CPU，还可以并行地、异步地使用它们。\n",
    "* 现代系统还拥有各种通信资源，如PCI Express、存储（通常是固态硬盘或网络存储）和网络带宽，为了达到最高效率可以并行使用它们。\n",
    "* 后端可以通过自动化地并行计算和通信来提高性能。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 在本节定义的`run`函数中执行了八个操作，并且操作之间没有依赖关系。设计一个实验，看看深度学习框架是否会自动地并行地执行它们。\n",
    "1. 当单个操作符的工作量足够小，即使在单个CPU或GPU上，并行化也会有所帮助。设计一个实验来验证这一点。\n",
    "1. 设计一个实验，在CPU和GPU这两种设备上使用并行计算和通信。\n",
    "1. 使用诸如NVIDIA的[Nsight](https://developer.nvidia.com/nsight-compute-2019_5)之类的调试器来验证代码是否有效。\n",
    "1. 设计并实验具有更加复杂的数据依赖关系的计算任务，以查看是否可以在提高性能的同时获得正确的结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa92ae",
   "metadata": {
    "origin_pos": 31,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/2795)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}