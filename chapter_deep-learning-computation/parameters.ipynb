{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef04b137",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5795fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U mxnet-cu101==1.7.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff88ca",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 参数管理\n",
    "\n",
    "在选择了架构并设置了超参数后，我们就进入了训练阶段。\n",
    "此时，我们的目标是找到使损失函数最小化的模型参数值。\n",
    "经过训练后，我们将需要使用这些参数来做出未来的预测。\n",
    "此外，有时我们希望提取参数，以便在其他环境中复用它们，\n",
    "将模型保存下来，以便它可以在其他软件中执行，\n",
    "或者为了获得科学的理解而进行检查。\n",
    "\n",
    "之前的介绍中，我们只依靠深度学习框架来完成训练的工作，\n",
    "而忽略了操作参数的具体细节。\n",
    "本节，我们将介绍以下内容：\n",
    "\n",
    "* 访问参数，用于调试、诊断和可视化；\n",
    "* 参数初始化；\n",
    "* 在不同模型组件间共享参数。\n",
    "\n",
    "(**我们首先看一下具有单隐藏层的多层感知机。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345a7282",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:55:58.336667Z",
     "iopub.status.busy": "2023-08-14T18:55:58.336144Z",
     "iopub.status.idle": "2023-08-14T18:56:00.079907Z",
     "shell.execute_reply": "2023-08-14T18:56:00.078768Z"
    },
    "origin_pos": 1,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:56:00] ../src/storage/storage.cc:196: Using Pooled (Naive) StorageManager for CPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0054572 ],\n",
       "       [0.00488594]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(8, activation='relu'))\n",
    "net.add(nn.Dense(1))\n",
    "net.initialize()  # 使用默认初始化方法\n",
    "\n",
    "X = np.random.uniform(size=(2, 4))\n",
    "net(X)  # 正向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3cad84",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "## [**参数访问**]\n",
    "\n",
    "我们从已有模型中访问参数。\n",
    "当通过`Sequential`类定义模型时，\n",
    "我们可以通过索引来访问模型的任意层。\n",
    "这就像模型是一个列表一样，每层的参数都在其属性中。\n",
    "如下所示，我们可以检查第二个全连接层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7fc7f95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.084468Z",
     "iopub.status.busy": "2023-08-14T18:56:00.083498Z",
     "iopub.status.idle": "2023-08-14T18:56:00.089517Z",
     "shell.execute_reply": "2023-08-14T18:56:00.088394Z"
    },
    "origin_pos": 6,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense1_ (\n",
      "  Parameter dense1_weight (shape=(1, 8), dtype=float32)\n",
      "  Parameter dense1_bias (shape=(1,), dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net[1].params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac319f09",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "输出的结果告诉我们一些重要的事情：\n",
    "首先，这个全连接层包含两个参数，分别是该层的权重和偏置。\n",
    "两者都存储为单精度浮点数（float32）。\n",
    "注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。\n",
    "\n",
    "### [**目标参数**]\n",
    "\n",
    "注意，每个参数都表示为参数类的一个实例。\n",
    "要对参数执行任何操作，首先我们需要访问底层的数值。\n",
    "有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。\n",
    "下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，\n",
    "提取后返回的是一个参数类实例，并进一步访问该参数的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c3141e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.094040Z",
     "iopub.status.busy": "2023-08-14T18:56:00.093371Z",
     "iopub.status.idle": "2023-08-14T18:56:00.099641Z",
     "shell.execute_reply": "2023-08-14T18:56:00.098625Z"
    },
    "origin_pos": 10,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mxnet.gluon.parameter.Parameter'>\n",
      "Parameter dense1_bias (shape=(1,), dtype=float32)\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "print(type(net[1].bias))\n",
    "print(net[1].bias)\n",
    "print(net[1].bias.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf678f74",
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "参数是复合的对象，包含值、梯度和额外信息。\n",
    "这就是我们需要显式参数值的原因。\n",
    "除了值之外，我们还可以访问每个参数的梯度。\n",
    "在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce631a8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.103517Z",
     "iopub.status.busy": "2023-08-14T18:56:00.102834Z",
     "iopub.status.idle": "2023-08-14T18:56:00.110065Z",
     "shell.execute_reply": "2023-08-14T18:56:00.109050Z"
    },
    "origin_pos": 15,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].weight.grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dcd25b",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "### [**一次性访问所有参数**]\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。\n",
    "当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，\n",
    "因为我们需要递归整个树来提取每个子块的参数。\n",
    "下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d865f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.113991Z",
     "iopub.status.busy": "2023-08-14T18:56:00.113374Z",
     "iopub.status.idle": "2023-08-14T18:56:00.118596Z",
     "shell.execute_reply": "2023-08-14T18:56:00.117802Z"
    },
    "origin_pos": 18,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense0_ (\n",
      "  Parameter dense0_weight (shape=(8, 4), dtype=float32)\n",
      "  Parameter dense0_bias (shape=(8,), dtype=float32)\n",
      ")\n",
      "sequential0_ (\n",
      "  Parameter dense0_weight (shape=(8, 4), dtype=float32)\n",
      "  Parameter dense0_bias (shape=(8,), dtype=float32)\n",
      "  Parameter dense1_weight (shape=(1, 8), dtype=float32)\n",
      "  Parameter dense1_bias (shape=(1,), dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net[0].collect_params())\n",
    "print(net.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8cb88",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68eaa134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.121897Z",
     "iopub.status.busy": "2023-08-14T18:56:00.121389Z",
     "iopub.status.idle": "2023-08-14T18:56:00.126726Z",
     "shell.execute_reply": "2023-08-14T18:56:00.125956Z"
    },
    "origin_pos": 22,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.collect_params()['dense1_bias'].data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b77315b",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "### [**从嵌套块收集参数**]\n",
    "\n",
    "让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。\n",
    "我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c00f9a4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.130114Z",
     "iopub.status.busy": "2023-08-14T18:56:00.129505Z",
     "iopub.status.idle": "2023-08-14T18:56:00.157050Z",
     "shell.execute_reply": "2023-08-14T18:56:00.156255Z"
    },
    "origin_pos": 27,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.3465841e-09, -1.1096747e-09,  6.4161774e-09,  6.6354131e-09,\n",
       "        -1.1265502e-09,  1.3285141e-10,  9.3619361e-09,  3.2229091e-09,\n",
       "         5.9429857e-09,  8.8181418e-09],\n",
       "       [-8.6219423e-09, -7.5150813e-10,  8.3133243e-09,  8.9321119e-09,\n",
       "        -1.6739999e-09,  3.2406069e-10,  1.2115976e-08,  4.4926454e-09,\n",
       "         8.0741742e-09,  1.2075874e-08]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(32, activation='relu'))\n",
    "    net.add(nn.Dense(16, activation='relu'))\n",
    "    return net\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for _ in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add(block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential()\n",
    "rgnet.add(block2())\n",
    "rgnet.add(nn.Dense(10))\n",
    "rgnet.initialize()\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790d771",
   "metadata": {
    "origin_pos": 31
   },
   "source": [
    "[**设计了网络后，我们看看它是如何工作的。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e21f55c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.160226Z",
     "iopub.status.busy": "2023-08-14T18:56:00.159675Z",
     "iopub.status.idle": "2023-08-14T18:56:00.164621Z",
     "shell.execute_reply": "2023-08-14T18:56:00.163824Z"
    },
    "origin_pos": 32,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Block.collect_params of Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Dense(4 -> 32, Activation(relu))\n",
      "      (1): Dense(32 -> 16, Activation(relu))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Dense(16 -> 32, Activation(relu))\n",
      "      (1): Dense(32 -> 16, Activation(relu))\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Dense(16 -> 32, Activation(relu))\n",
      "      (1): Dense(32 -> 16, Activation(relu))\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Dense(16 -> 32, Activation(relu))\n",
      "      (1): Dense(32 -> 16, Activation(relu))\n",
      "    )\n",
      "  )\n",
      "  (1): Dense(16 -> 10, linear)\n",
      ")>\n",
      "sequential1_ (\n",
      "  Parameter dense2_weight (shape=(32, 4), dtype=float32)\n",
      "  Parameter dense2_bias (shape=(32,), dtype=float32)\n",
      "  Parameter dense3_weight (shape=(16, 32), dtype=float32)\n",
      "  Parameter dense3_bias (shape=(16,), dtype=float32)\n",
      "  Parameter dense4_weight (shape=(32, 16), dtype=float32)\n",
      "  Parameter dense4_bias (shape=(32,), dtype=float32)\n",
      "  Parameter dense5_weight (shape=(16, 32), dtype=float32)\n",
      "  Parameter dense5_bias (shape=(16,), dtype=float32)\n",
      "  Parameter dense6_weight (shape=(32, 16), dtype=float32)\n",
      "  Parameter dense6_bias (shape=(32,), dtype=float32)\n",
      "  Parameter dense7_weight (shape=(16, 32), dtype=float32)\n",
      "  Parameter dense7_bias (shape=(16,), dtype=float32)\n",
      "  Parameter dense8_weight (shape=(32, 16), dtype=float32)\n",
      "  Parameter dense8_bias (shape=(32,), dtype=float32)\n",
      "  Parameter dense9_weight (shape=(16, 32), dtype=float32)\n",
      "  Parameter dense9_bias (shape=(16,), dtype=float32)\n",
      "  Parameter dense10_weight (shape=(10, 16), dtype=float32)\n",
      "  Parameter dense10_bias (shape=(10,), dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet.collect_params)\n",
    "print(rgnet.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40ffa5",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。\n",
    "下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96877def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.167800Z",
     "iopub.status.busy": "2023-08-14T18:56:00.167288Z",
     "iopub.status.idle": "2023-08-14T18:56:00.172779Z",
     "shell.execute_reply": "2023-08-14T18:56:00.172009Z"
    },
    "origin_pos": 36,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4daeef5",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "## 参数初始化\n",
    "\n",
    "知道了如何访问参数后，现在我们看看如何正确地初始化参数。\n",
    "我们在 :numref:`sec_numerical_stability`中讨论了良好初始化的必要性。\n",
    "深度学习框架提供默认随机初始化，\n",
    "也允许我们创建自定义初始化方法，\n",
    "满足我们通过其他规则实现初始化权重。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb958895",
   "metadata": {
    "origin_pos": 41,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "默认情况下，MXNet通过初始化权重参数的方法是\n",
    "从均匀分布$U(-0.07, 0.07)$中随机采样权重，并将偏置参数设置为0。\n",
    "MXNet的`init`模块提供了多种预置初始化方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c7b488",
   "metadata": {
    "origin_pos": 45
   },
   "source": [
    "### [**内置初始化**]\n",
    "\n",
    "让我们首先调用内置的初始化器。\n",
    "下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，\n",
    "且将偏置参数设置为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64da91a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.176132Z",
     "iopub.status.busy": "2023-08-14T18:56:00.175599Z",
     "iopub.status.idle": "2023-08-14T18:56:00.184089Z",
     "shell.execute_reply": "2023-08-14T18:56:00.183301Z"
    },
    "origin_pos": 46,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00324057, -0.00895028, -0.00698632,  0.01030831])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里的force_reinit确保参数会被重新初始化，不论之前是否已经被初始化\n",
    "net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ea1780",
   "metadata": {
    "origin_pos": 50
   },
   "source": [
    "我们还可以将所有参数初始化为给定的常数，比如初始化为1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b0682cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.187268Z",
     "iopub.status.busy": "2023-08-14T18:56:00.186744Z",
     "iopub.status.idle": "2023-08-14T18:56:00.195395Z",
     "shell.execute_reply": "2023-08-14T18:56:00.194582Z"
    },
    "origin_pos": 51,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize(init=init.Constant(1), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6284e",
   "metadata": {
    "origin_pos": 55
   },
   "source": [
    "我们还可以[**对某些块应用不同的初始化方法**]。\n",
    "例如，下面我们使用Xavier初始化方法初始化第一个神经网络层，\n",
    "然后将第三个神经网络层初始化为常量值42。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e74ce05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.198610Z",
     "iopub.status.busy": "2023-08-14T18:56:00.198094Z",
     "iopub.status.idle": "2023-08-14T18:56:00.205979Z",
     "shell.execute_reply": "2023-08-14T18:56:00.205190Z"
    },
    "origin_pos": 56,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.17594433  0.02314097 -0.1992535   0.09509248]\n",
      "[[42. 42. 42. 42. 42. 42. 42. 42.]]\n"
     ]
    }
   ],
   "source": [
    "net[0].weight.initialize(init=init.Xavier(), force_reinit=True)\n",
    "net[1].initialize(init=init.Constant(42), force_reinit=True)\n",
    "print(net[0].weight.data()[0])\n",
    "print(net[1].weight.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7516540",
   "metadata": {
    "origin_pos": 60
   },
   "source": [
    "### [**自定义初始化**]\n",
    "\n",
    "有时，深度学习框架没有提供我们需要的初始化方法。\n",
    "在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd5c7a",
   "metadata": {
    "origin_pos": 61,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "在这里，我们定义了`Initializer`类的子类。\n",
    "通常，我们只需要实现`_init_weight`函数，\n",
    "该函数接受张量参数（`data`）并为其分配所需的初始化值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53e43835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.209262Z",
     "iopub.status.busy": "2023-08-14T18:56:00.208740Z",
     "iopub.status.idle": "2023-08-14T18:56:00.220513Z",
     "shell.execute_reply": "2023-08-14T18:56:00.219704Z"
    },
    "origin_pos": 65,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init dense0_weight (8, 4)\n",
      "Init dense1_weight (1, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.       , -0.       , -0.       ,  8.522827 ],\n",
       "       [ 0.       , -8.828651 , -0.       , -5.6012006]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyInit(init.Initializer):\n",
    "    def _init_weight(self, name, data):\n",
    "        print('Init', name, data.shape)\n",
    "        data[:] = np.random.uniform(-10, 10, data.shape)\n",
    "        data *= np.abs(data) >= 5\n",
    "\n",
    "net.initialize(MyInit(), force_reinit=True)\n",
    "net[0].weight.data()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4e77c4",
   "metadata": {
    "origin_pos": 69
   },
   "source": [
    "注意，我们始终可以直接设置参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eee1ad2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.223635Z",
     "iopub.status.busy": "2023-08-14T18:56:00.223120Z",
     "iopub.status.idle": "2023-08-14T18:56:00.229789Z",
     "shell.execute_reply": "2023-08-14T18:56:00.229008Z"
    },
    "origin_pos": 70,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42.      ,  1.      ,  1.      ,  9.522827])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data()[:] += 1\n",
    "net[0].weight.data()[0, 0] = 42\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c89120",
   "metadata": {
    "origin_pos": 74,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "高级用户请注意：如果要在`autograd`范围内调整参数，\n",
    "则需要使用`set_data`，以避免误导自动微分机制。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e5889",
   "metadata": {
    "origin_pos": 75
   },
   "source": [
    "## [**参数绑定**]\n",
    "\n",
    "有时我们希望在多个层间共享参数：\n",
    "我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "464edad1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T18:56:00.233072Z",
     "iopub.status.busy": "2023-08-14T18:56:00.232548Z",
     "iopub.status.idle": "2023-08-14T18:56:00.247433Z",
     "shell.execute_reply": "2023-08-14T18:56:00.246642Z"
    },
    "origin_pos": 76,
    "tab": [
     "mxnet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True]\n",
      "[ True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Dense(8, activation='relu')\n",
    "net.add(nn.Dense(8, activation='relu'),\n",
    "        shared,\n",
    "        nn.Dense(8, activation='relu', params=shared.params),\n",
    "        nn.Dense(10))\n",
    "net.initialize()\n",
    "\n",
    "X = np.random.uniform(size=(2, 20))\n",
    "net(X)\n",
    "\n",
    "# 检查参数是否相同\n",
    "print(net[1].weight.data()[0] == net[2].weight.data()[0])\n",
    "net[1].weight.data()[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[1].weight.data()[0] == net[2].weight.data()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b87a1",
   "metadata": {
    "origin_pos": 80,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "这个例子表明第二层和第三层的参数是绑定的。\n",
    "它们不仅值相等，而且由相同的张量表示。\n",
    "因此，如果我们改变其中一个参数，另一个参数也会改变。\n",
    "这里有一个问题：当参数绑定时，梯度会发生什么情况？\n",
    "答案是由于模型参数包含梯度，\n",
    "因此在反向传播期间第二个隐藏层和第三个隐藏层的梯度会加在一起。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7fc69b",
   "metadata": {
    "origin_pos": 82
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 使用 :numref:`sec_model_construction` 中定义的`FancyMLP`模型，访问各个层的参数。\n",
    "1. 查看初始化模块文档以了解不同的初始化方法。\n",
    "1. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。\n",
    "1. 为什么共享参数是个好主意？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe67dc",
   "metadata": {
    "origin_pos": 83,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1831)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}